{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sirius-Aabhas/CS69002_9A_18CS60R55/blob/master/DL_Assign.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bRuGSGqnMOXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e66b2859-b9b0-4b41-d035-9721bc7bebc2"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data_utils\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = torch.device('cuda:0')\n",
        "device"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "ijrEth3MRQ6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fetching data"
      ]
    },
    {
      "metadata": {
        "id": "J5ITVAYmiy5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQLFDGIQjrh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(uploaded), uploaded.keys(), type(uploaded['Train_20K.csv'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7sqK9HKoGzv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.StringIO(uploaded['Train_20K.csv'].decode('utf-8')), sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x3VpG5_h1oKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fetching data for now"
      ]
    },
    {
      "metadata": {
        "id": "pHXKubc019hc",
        "colab_type": "code",
        "outputId": "3a10dd2f-537c-4f51-d713-44daf5a116a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Train_20K.csv', sep='\\t')\n",
        "df.tail()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17994</th>\n",
              "      <td>I was pleasantly surprised by the film. Let's ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17995</th>\n",
              "      <td>you must be seeing my comments over many films...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17996</th>\n",
              "      <td>This is one of those movies that they did too ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17997</th>\n",
              "      <td>Anyone notice that Tommy only has 3 facial exp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17998</th>\n",
              "      <td>I remember watching ATTACK when it first came ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "17994  I was pleasantly surprised by the film. Let's ...      1\n",
              "17995  you must be seeing my comments over many films...      0\n",
              "17996  This is one of those movies that they did too ...      1\n",
              "17997  Anyone notice that Tommy only has 3 facial exp...      0\n",
              "17998  I remember watching ATTACK when it first came ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Gpv07aqq0wJX",
        "colab_type": "code",
        "outputId": "d71f872d-fa2e-4882-802a-cf6c03370f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('Test_5K.csv', sep='\\t')\n",
        "df_test.tail()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>I don't know how to describe this movie. It's ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>I found this movie hilarious. The spoofs on ot...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>My family and I have viewed this movie often o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>The Shining, you know what's weird about this ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>Nobody could like this movie for its merit but...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "4995  I don't know how to describe this movie. It's ...      0\n",
              "4996  I found this movie hilarious. The spoofs on ot...      1\n",
              "4997  My family and I have viewed this movie often o...      1\n",
              "4998  The Shining, you know what's weird about this ...      1\n",
              "4999  Nobody could like this movie for its merit but...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "zvXSxRtMNw8U",
        "colab_type": "code",
        "outputId": "a47c795a-1258-4593-c93c-ace9867739a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df[df['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df[df['label']==1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 8994\n",
            "Number of Positive movie reviews 9005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_rbQhm8mQ7BN",
        "colab_type": "code",
        "outputId": "07b00408-991b-4048-b52f-577d3d53715c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "text_reviews = df['text'].astype(str).tolist()\n",
        "text_labels = df['label'].astype(int)\n",
        "\n",
        "text_reviews = [x.lower() for x in text_reviews]\n",
        "\n",
        "filtered_text_reviews = []\n",
        "for sent in text_reviews:\n",
        "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_text_reviews.append(' '.join([w for w in word_tokens if not w in stop_words]))\n",
        "            \n",
        "text_reviews = filtered_text_reviews\n",
        "print(text_reviews[0], text_labels[0])\n",
        "print(len(text_reviews), len(text_labels))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "john waters given us genuinely enjoyable film certainly isnt without shocking watersesque moments tamer older culty stuff pink flamingoes pecker harkens back johns early mainstream stage reminds viewer kind humor evident polyester overall really fun comedy great moments 1\n",
            "17999 17999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tOgBzlyA2znw",
        "colab_type": "code",
        "outputId": "3f433029-d2b3-4090-f284-20b6095086a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "cell_type": "code",
      "source": [
        "data = [(text_reviews[i], text_labels[i])for i in range(len(text_labels))]\n",
        "data[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('john water given u genuinely enjoyable film certainly isnt without shocking watersesque moment tamer older culty stuff pink flamingo pecker harkens back john early mainstream stage reminds viewer kind humor evident polyester overall really fun comedy great moment',\n",
              "  1),\n",
              " ('first two season comedy series strange werent funny drama element bill mother struggling usual problem life element bit depressing didnt mix well th comedy element probably dropped soon became one funniest comedy series bbc ever made chemistry bill ben character funny always many brilliant memorable sketch series christmas special hilarious real treat christmas br br show came stop main actor gary olsen playing bill passed away sad brilliant actor film n funny man ripbr br underrated show sadly disappeared television screen doesnt repeated often though appear uktv gold repeated bbc one two show brilliant comedy new audience',\n",
              "  1),\n",
              " ('unfortunate mess shiner wanted like overthetop antifilm aspirant fact found number moment powerful resonance sadly moment far appreciate calson attempting advantage aspired bare bone budget cinematography destroyed truly atrocious editing benefited movie allbr br bad acting abounds low budget big budget cinema shiner remarkably bad performance nearly painful watch particular straight couple linda young guy two poorly written character offering almost nothing story acting abysmal neither actor seems capable resisting smirking cracking drearily drop line appalling lack skill choppy editing almost lends feeling role entirely gratuitous dropped avoid film stereotypically cast oddball gay film would better suchbr br going wrong several performance seem capture calson hoping get particular story centering bob tim two richly drawn character offer reward genuinely captivating performance nicholas king bob david zelinas tim tim boxer serious issue remarkably low self esteem disguised almost cartoon like arrogance wear like armour plating obsessed tim seemingly harmless yet ultimately creepy bob stalk boxer classic catandmouse fashion table turned hunter becomes hunted resulting film genuine emotional catharsis film artificially hardedged thats compliment one character must revelatory break breakdown case prof final confrontation bob tim provide zelinas king opportunity display real acting chopsbr br played scott stepp derris nile tony danny seem focus movie despite bravado moment including one truly disturbing scene revealing sexviolence obsession cant seem escape cartoonlike artifice difficult look beyond seeming one note symphony find anything obviousbr br ultimately raw material could used tell story better fashion ala really isnt much recommend yet performance messrs king zelinas really offer something special glimpse might ultimately worth seeing',\n",
              "  0),\n",
              " ('im entirely sure rob schmidt qualifies master genre horror since previously directed one horror film called wrong turn one actually slightly mediocre fact made right die one best creepiest episode entire second season master horror franchise similar underdog story season one william malone made best episode fair haired child even though long feature film fear dot com house haunted hill sucked pretty badlybr br story right die cleverly pick nowadays piping hot social debate euthanasia thankfully also feature multiple oldfashioned horror theme like ghostly vengeance murderous conspiracy pitch black humor comic book styled violence whilst driving home late one night discussing husband continuous adultery addison couple involved terrible car accident cliff walk away wreck unharmed wife abby fully burned need kept alive artificially whilst cliff sleazy attorney corbin bernsen dentist want plug plug sue car constructor abbey mum set giant medium campaign keep daughter alive vegetable blame everything cliff meanwhile abbey hateful spirit come back revenge kill someone cliff surrounding whenever near fatal experience medical device victim cliff realizes might safer keep wife alive want remain alive well right die stupendous episode exactly type stuff always hoped see tvseries concept like master horror violent gory sick twisted sense humor load sleaze sequence euthanasia theme whole obligatory medium circus surround processed script well yet without unnecessarily reverting political standpoint morality lesson atmosphere suspenseful killing sequence suitably nasty unsettling actress julia anderson robin sydney pretty face impressively voluptuous rack always welcome plus corbin bernsen finally offered chance depict meanspirited egocentric bastard great moh episode definitely one highlight season',\n",
              "  1),\n",
              " ('wasnt sure whether laugh cry porretta good looking resembled like mexican porn star english outlaw costume costume tshirt strip black leather marions clothesor lack themthat really got fan stinker really believe woman dressed like medieval england mongol viking inaccurate stupid episode alien worst especially make mainly consisted oatmeal facean old trickthe hedgehog monster pretty funny climbing side castle ladder arrowsas u accent grated initial drawling voice rawbin hood liddle johnthe second robin marion really quite minging look left show went totally pan',\n",
              "  0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "0V2GnN9Z6Yj4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwEQFKUT6cc8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "type(uploaded), uploaded.keys(), type(uploaded['Test_5K.csv'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qs2rHjSN6k2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(io.StringIO(uploaded['Test_5K.csv'].decode('utf-8')), sep='\\t')\n",
        "df_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTk26HxZ6qjj",
        "colab_type": "code",
        "outputId": "321fe41f-0c97-49df-dc7b-10a66d200f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of Negative movie reviews', len(df_test[df_test['label']==0]))\n",
        "print('Number of Positive movie reviews', len(df_test[df_test['label']==1]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Negative movie reviews 2482\n",
            "Number of Positive movie reviews 2518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qirQBcpl60vS",
        "colab_type": "code",
        "outputId": "4423890e-8cd2-498c-e6ba-35c2fa944c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "text_reviews_test = df_test['text'].astype(str).tolist()\n",
        "text_labels_test = df_test['label'].astype(int)\n",
        "\n",
        "text_reviews_test = [x.lower() for x in text_reviews_test]\n",
        "\n",
        "filtered_text_reviews = []\n",
        "for sent in text_reviews_test:\n",
        "    sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_text_reviews.append(' '.join([w for w in word_tokens if not w in stop_words]))\n",
        "            \n",
        "text_reviews_test = filtered_text_reviews\n",
        "\n",
        "print(text_reviews_test[0], text_labels_test[0])\n",
        "print(len(text_reviews_test), len(text_labels_test))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "always inaccurate picture homeless tv told lot lies panhandlers early 1990s made everyone look bad claimed made 100 day 2040 day much closer reality someone drove held sign offering work offered work actually went took work physically ableand would offered 100000 id damned sure invested apt prepaid least 2 years kept bank still left 1020000 nl 12 25 cash games casinos usually always win could win decent bankroll instead win 1000 month playing always minimum buying due wanting risk losing homeless cause didnt wan na risk spending money going broke sometimes 10002000 sock slept outside anyone wanting talk contact sevencard2003 yahoo messengeri admit different homeless people though due fact never drank smoke took drugs im longer homeless govt housing 177 month getting ssi spend time winning online poker mom sunflower diversified worked hard get ssi glad days hiding stage convention center casino night sleeping worrying getting caught security finally tv crew picked theyd lot sooner shame dont better select pick 0\n",
            "5000 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "89gIvwYSx_MJ",
        "outputId": "1d90c6db-c924-42c6-c13e-b91a61e62f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "cell_type": "code",
      "source": [
        "data_test = [(text_reviews_test[i], text_labels_test[i])for i in range(len(text_labels_test))]\n",
        "data_test[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('always inaccurate picture homeless tv told lot lie panhandler early 1990s made everyone look bad claimed made 100 day 2040 day much closer reality someone drove held sign offering work offered work actually went took work physically ableand would offered 100000 id damned sure invested apt prepaid least 2 year kept bank still left 1020000 nl 12 25 cash game casino usually always win could win decent bankroll instead win 1000 month playing always minimum buying due wanting risk losing homeless cause didnt wan na risk spending money going broke sometimes 10002000 sock slept outside anyone wanting talk contact sevencard2003 yahoo messengeri admit different homeless people though due fact never drank smoke took drug im longer homeless govt housing 177 month getting ssi spend time winning online poker mom sunflower diversified worked hard get ssi glad day hiding stage convention center casino night sleeping worrying getting caught security finally tv crew picked theyd lot sooner shame dont better select pick',\n",
              "  0),\n",
              " ('moviemakers even preview released script jump place place without giving much explanation beginning doesnt clarify prequel start superman beginning earth jump point last movie doesnt really alert viewer confusing superman weak need prozac portrayed potential homewrecker stalker someone clearly depressed confused type character rarely make interesting hero ending absolutely ridiculous superman ending hospital made want kill im seriously waiting snl skit superman appears maury povich maury say result case child superman father sum ok acting superman kevin spacey horrible script movie basically unwatchable',\n",
              "  0),\n",
              " ('heavily reedited often confusing original screen version man fire least ten year date made passing year havent made better kind movie producer much money little experience make get attention everyone else pay outstanding alimony drug dealer scott glenn bodyguard going limb rescue 12yearold charge kidnapped daughter wealthy italian family interesting cast joe pesci brooke adam danny aiello jonathan pryce done better action sluggish sparse john scott exceptionally fine score part turned last reel die hard make positive impression one case remake made tony scott original choice director version improvement',\n",
              "  0),\n",
              " ('notice people think film speaks truth either born moon landing 19691972 old enough appreciate think much easier question historic event live itbr br youngster time apollo old enough understand going entire world followed moon landing family gathered around tv watch launch newspaper headline screamed latest goingson day launch landing moonwalk moon liftoff way splashdown multitude language school class cancelled could watch main event tv apollo 13 world prayed held collective breath men limped home uncertain fate couldnt go anywhere without someone asking latest world truly one community br br buffer 30odd year fact easy claim fraud worldwide enthusiasm interest died left history book anybody claim history wrong attempt prove bunch lie madeup fact completely ignoring preponderance evidence showing otherwisenot mention proof dwells soul memory lived wonderfully heady fantastic day',\n",
              "  0),\n",
              " ('first lowbudget movie expectation incredibly low going assume people looking info movie wanted bloodfest essentially thats isbr br plot really none basically saw china whole hell lot worse cast none period special effect absolutely awful opinion cutaway blood often completely unbelievable amount splatter color texture etcbr br believe purpose movie supposed brutal shock film great potential bigger budget poor scripting poor dialogue awful acting seemed like camcorder video shot plain unbelievable gore made movie truly awfulbr br movie worth taking chance review even brate movie deserve opportunity blood trail example recent saw review worth simply awful hope people considering movie read comment decide itbr br im brutality shock overall unrealism truly awful acting make awful experience save timemoney chance something else wont disappointed',\n",
              "  0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "zydP-RKS0GgB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating Bag Of Word (BOW) representation of sentences."
      ]
    },
    {
      "metadata": {
        "id": "COY4uTBU2OSj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_word_ids(dataset):\n",
        "    word_to_ix = {}\n",
        "    word_cntr = {}\n",
        "    word_set = set()\n",
        "    #print(dataset)\n",
        "    for sent in dataset:\n",
        "        for word in sent.split():\n",
        "            if word not in word_cntr:\n",
        "                word_cntr[word] = 1\n",
        "            else:\n",
        "                word_cntr[word] += 1\n",
        "    \n",
        "    for word in word_cntr:\n",
        "        if word_cntr[word] >= 10:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "            \n",
        "    word_to_ix['<UNKNOWN>'] = len(word_to_ix)\n",
        "        \n",
        "    return word_to_ix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXGW8ypdRXsQ",
        "colab_type": "code",
        "outputId": "52eaeb45-70e6-4745-90ac-de61995addec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_ix = generate_word_ids(text_reviews + text_reviews_test)\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "VOCAB_SIZE"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "Bd_AuK5Xy679",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    # create a vector of zeros of vocab size = len(word_to_idx)\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_ix:\n",
        "            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')\n",
        "            vec[word_to_ix['<UNKNOWN>']]+=1\n",
        "        else:\n",
        "            vec[word_to_ix[word]]+=1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "def make_target(label):\n",
        "    return torch.LongTensor([label])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8jOfEzAXgMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = torch.ones(1,10)\n",
        "b = torch.ones(1,10)\n",
        "a = torch.cat((a,b))\n",
        "d = torch.ones(1,10)\n",
        "torch.cat((a,d))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7XiuOLoWQlh",
        "colab_type": "code",
        "outputId": "da8164f8-002e-47df-d289-7b9ac02774a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "cell_type": "code",
      "source": [
        "t1 = time.time()\n",
        "features = make_bow_vector(text_reviews[0], word_to_ix).to(device)\n",
        "for i in range(1,len(text_reviews)):\n",
        "    vec = make_bow_vector(text_reviews[i], word_to_ix).to(device)\n",
        "    features = torch.cat((features,vec)).to(device)\n",
        "    if i%1000 == 0:\n",
        "        print(time.time() - t1)\n",
        "        t1 = time.time()\n",
        "        print(features.shape)\n",
        "targets = torch.tensor(text_labels, dtype=torch.int).to(device)\n",
        "\n",
        "print(time.time() - t1)\n",
        "print(features.shape)\n",
        "print(targets.shape)\n",
        "train = data_utils.TensorDataset(features, targets)\n",
        "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.9824812412261963\n",
            "torch.Size([1001, 20099])\n",
            "2.570889472961426\n",
            "torch.Size([2001, 20099])\n",
            "3.6018967628479004\n",
            "torch.Size([3001, 20099])\n",
            "4.937259674072266\n",
            "torch.Size([4001, 20099])\n",
            "6.272442102432251\n",
            "torch.Size([5001, 20099])\n",
            "7.5886149406433105\n",
            "torch.Size([6001, 20099])\n",
            "40.52172803878784\n",
            "torch.Size([7001, 20099])\n",
            "60.44947385787964\n",
            "torch.Size([8001, 20099])\n",
            "68.29971933364868\n",
            "torch.Size([9001, 20099])\n",
            "76.67607975006104\n",
            "torch.Size([10001, 20099])\n",
            "84.54345989227295\n",
            "torch.Size([11001, 20099])\n",
            "92.83307480812073\n",
            "torch.Size([12001, 20099])\n",
            "100.64534759521484\n",
            "torch.Size([13001, 20099])\n",
            "109.59531545639038\n",
            "torch.Size([14001, 20099])\n",
            "117.1657247543335\n",
            "torch.Size([15001, 20099])\n",
            "125.73737573623657\n",
            "torch.Size([16001, 20099])\n",
            "133.45578980445862\n",
            "torch.Size([17001, 20099])\n",
            "142.55526113510132\n",
            "torch.Size([17999, 20099])\n",
            "torch.Size([17999])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B2BNlqrAJleH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task1"
      ]
    },
    {
      "metadata": {
        "id": "m9i8PVtgz0NS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model1A(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model1A, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 50)\n",
        "        self.lin2 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xu-8rq-AJj0K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model1B(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model1B, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 100)\n",
        "        self.lin2 = nn.Linear(100, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3EY8WlEKQKE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model1C(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model1C, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 200)\n",
        "        self.lin2 = nn.Linear(200, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "44ZSyECsKSpZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model1D(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model1D, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 500)\n",
        "        self.lin2 = nn.Linear(500, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aboTbOIuyVd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1A = Model1A(VOCAB_SIZE)\n",
        "model1B = Model1B(VOCAB_SIZE)\n",
        "model1C = Model1C(VOCAB_SIZE)\n",
        "model1D = Model1D(VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJgvIuflzD7S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for param in model1A.parameters():\n",
        "    print(param,param.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eXZ5oiJz3oRs"
      },
      "cell_type": "markdown",
      "source": [
        "##Task2"
      ]
    },
    {
      "metadata": {
        "id": "AC5vnfLG3s8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model2A(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model2A, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 10)\n",
        "        self.lin2 = nn.Linear(10, 10)\n",
        "        self.lin3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uAl-p3ST3svP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model2B(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model2B, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 20)\n",
        "        self.lin2 = nn.Linear(20, 10)\n",
        "        self.lin3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8gI_C_AC3srz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model2C(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model2C, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 30)\n",
        "        self.lin2 = nn.Linear(30, 30)\n",
        "        self.lin3 = nn.Linear(30, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xeu29fbI3sjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model2D(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model2D, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 50)\n",
        "        self.lin2 = nn.Linear(50, 50)\n",
        "        self.lin3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWWVSG8z4rnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model2E(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model2E, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 100)\n",
        "        self.lin2 = nn.Linear(100, 50)\n",
        "        self.lin3 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "y_MUqcWjpbBA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2A = Model2A(VOCAB_SIZE)\n",
        "model2B = Model2B(VOCAB_SIZE)\n",
        "model2C = Model2C(VOCAB_SIZE)\n",
        "model2D = Model2D(VOCAB_SIZE)\n",
        "model2E = Model2E(VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gcyPaqzo42Ea"
      },
      "cell_type": "markdown",
      "source": [
        "##Task3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Kire3o2W42Ed",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model3A(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model3A, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 100)\n",
        "        self.lin2 = nn.Linear(100, 50)\n",
        "        self.lin3 = nn.Linear(50, 10)\n",
        "        self.lin4 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.lin3(x)\n",
        "        return F.softmax(self.lin4(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hPQL-96z42Eq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model3B(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model3B, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 200)\n",
        "        self.lin2 = nn.Linear(200, 100)\n",
        "        self.lin3 = nn.Linear(100, 10)\n",
        "        self.lin4 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.lin1(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.lin3(x)\n",
        "        return F.softmax(self.lin4(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6HYfG8pv2Wy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model3A = Model3A(VOCAB_SIZE)\n",
        "model3B = Model3B(VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pGje3Qmb53SB"
      },
      "cell_type": "markdown",
      "source": [
        "##Task4"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1q9dRGoH53SF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model4A(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model4A, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 30)\n",
        "        self.lin2 = nn.Linear(30, 20)\n",
        "        self.drop_layer = nn.Dropout(p=p)\n",
        "        self.lin3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = F.softmax(self.lin1(x))\n",
        "        x = F.softmax(self.lin2(x))\n",
        "        x = self.drop_layer(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Cxk52Poe53SN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model4B(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model4B, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 100)\n",
        "        self.drop_layer = nn.Dropout(p=p)\n",
        "        self.lin2 = nn.Linear(100, 100)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = F.softmax(self.lin1(x))\n",
        "        x = self.drop_layer(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7f8iUrh6K5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model4C(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model4C, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 100)\n",
        "        self.drop_layer = nn.Dropout(p=p)\n",
        "        self.lin2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = F.softmax(self.lin1(x))\n",
        "        x = self.drop_layer(x)\n",
        "        return F.softmax(self.lin2(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_P3kpbn6CkV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Task5"
      ]
    },
    {
      "metadata": {
        "id": "WyuNEDLy6F-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model5A(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model5A, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 30)\n",
        "        self.lin2 = nn.Linear(30, 20)\n",
        "        self.drop_layer = nn.Dropout(p=p)\n",
        "        self.lin3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.drop_layer(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jvAMDPl6IDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model5B(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Model5B, self).__init__()\n",
        "        self.lin1 = nn.Linear(vocab_size, 30)\n",
        "        self.lin2 = nn.Linear(30, 20)\n",
        "        self.drop_layer = nn.Dropout(p=p)\n",
        "        self.lin3 = nn.Linear(20, 10)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = F.t(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.drop_layer(x)\n",
        "        return F.softmax(self.lin3(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kdBYZH5C6H-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9q06ioWi6H4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUigOUZZ6Hwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMIoCKTv2oJC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run code"
      ]
    },
    {
      "metadata": {
        "id": "I8yGFyUV-ykT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, isGpu):\n",
        "    if isGpu:\n",
        "        model = model.to(device)\n",
        "    # define a loss function and an optimizer\n",
        "    loss_function = nn.NLLLoss()\n",
        "    opt = torch.optim.SGD(model.parameters(), lr = 0.0001)\n",
        "    # the training loop\n",
        "    for epoch in range(500):\n",
        "        tic = time.time()\n",
        "        for batch_idx, (instance, label) in enumerate(train_loader):\n",
        "            # get the training data\n",
        "            model.zero_grad()\n",
        "            label = label.long()\n",
        "            probs = model(instance) # forward pass\n",
        "            loss = loss_function(probs, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        toc = time.time()\n",
        "        print('EPOCH: {}, CURRENT LOSS: {}, TIME TAKEN: {}'.format(epoch, loss.data, (toc-tic)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ovWIpc5K42Ff",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(model):\n",
        "    model = model.cpu()\n",
        "    preds = []\n",
        "    for instance, label in data_test:\n",
        "        bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
        "        logprobs = model(bow_vec)\n",
        "        #print(logprobs)\n",
        "        pred = np.argmax(logprobs.data.cpu().numpy())\n",
        "        preds.append(pred)\n",
        "    print('accuracy: {}'.format(accuracy_score(text_labels_test, preds)))\n",
        "    print('precision: {}'.format(precision_score(text_labels_test, preds)))\n",
        "    print('recall: {}'.format(recall_score(text_labels_test, preds)))\n",
        "    print('f1-score: {}'.format(f1_score(text_labels_test, preds)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "--MMxg1r42FR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_and_save(model): \n",
        "    train(model, True)\n",
        "    validate(model)\n",
        "    torch.save(model, model._get_name()+'.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZpqSk7IAc9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4220
        },
        "outputId": "4ad07800-605e-448d-a010-7ef6959b0488"
      },
      "cell_type": "code",
      "source": [
        "run_and_save(model3B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0, CURRENT LOSS: -0.5084578394889832, TIME TAKEN: 0.5586252212524414\n",
            "EPOCH: 1, CURRENT LOSS: -0.5009728074073792, TIME TAKEN: 0.4584496021270752\n",
            "EPOCH: 2, CURRENT LOSS: -0.5052897334098816, TIME TAKEN: 0.4567406177520752\n",
            "EPOCH: 3, CURRENT LOSS: -0.5051054358482361, TIME TAKEN: 0.45734214782714844\n",
            "EPOCH: 4, CURRENT LOSS: -0.5009975433349609, TIME TAKEN: 0.456650972366333\n",
            "EPOCH: 5, CURRENT LOSS: -0.49965378642082214, TIME TAKEN: 0.45360255241394043\n",
            "EPOCH: 6, CURRENT LOSS: -0.502781867980957, TIME TAKEN: 0.454648494720459\n",
            "EPOCH: 7, CURRENT LOSS: -0.4991413950920105, TIME TAKEN: 0.45733046531677246\n",
            "EPOCH: 8, CURRENT LOSS: -0.4971586763858795, TIME TAKEN: 0.4542970657348633\n",
            "EPOCH: 9, CURRENT LOSS: -0.5065040588378906, TIME TAKEN: 0.45407867431640625\n",
            "EPOCH: 10, CURRENT LOSS: -0.5010671019554138, TIME TAKEN: 0.4538424015045166\n",
            "EPOCH: 11, CURRENT LOSS: -0.5020821690559387, TIME TAKEN: 0.45279431343078613\n",
            "EPOCH: 12, CURRENT LOSS: -0.501261293888092, TIME TAKEN: 0.4558145999908447\n",
            "EPOCH: 13, CURRENT LOSS: -0.5034490823745728, TIME TAKEN: 0.4556732177734375\n",
            "EPOCH: 14, CURRENT LOSS: -0.5006234645843506, TIME TAKEN: 0.45511937141418457\n",
            "EPOCH: 15, CURRENT LOSS: -0.5066167712211609, TIME TAKEN: 0.45594048500061035\n",
            "EPOCH: 16, CURRENT LOSS: -0.5032438635826111, TIME TAKEN: 0.45549774169921875\n",
            "EPOCH: 17, CURRENT LOSS: -0.5005176067352295, TIME TAKEN: 0.454892635345459\n",
            "EPOCH: 18, CURRENT LOSS: -0.5041464567184448, TIME TAKEN: 0.4567081928253174\n",
            "EPOCH: 19, CURRENT LOSS: -0.5006592273712158, TIME TAKEN: 0.4548912048339844\n",
            "EPOCH: 20, CURRENT LOSS: -0.5007280111312866, TIME TAKEN: 0.45301032066345215\n",
            "EPOCH: 21, CURRENT LOSS: -0.49944716691970825, TIME TAKEN: 0.4546222686767578\n",
            "EPOCH: 22, CURRENT LOSS: -0.5018137097358704, TIME TAKEN: 0.4565548896789551\n",
            "EPOCH: 23, CURRENT LOSS: -0.5022287964820862, TIME TAKEN: 0.453948974609375\n",
            "EPOCH: 24, CURRENT LOSS: -0.5036510229110718, TIME TAKEN: 0.4567399024963379\n",
            "EPOCH: 25, CURRENT LOSS: -0.5049974322319031, TIME TAKEN: 0.4538888931274414\n",
            "EPOCH: 26, CURRENT LOSS: -0.5025420784950256, TIME TAKEN: 0.4545571804046631\n",
            "EPOCH: 27, CURRENT LOSS: -0.49965229630470276, TIME TAKEN: 0.45339250564575195\n",
            "EPOCH: 28, CURRENT LOSS: -0.5028822422027588, TIME TAKEN: 0.45458078384399414\n",
            "EPOCH: 29, CURRENT LOSS: -0.5083097815513611, TIME TAKEN: 0.4499626159667969\n",
            "EPOCH: 30, CURRENT LOSS: -0.5031201243400574, TIME TAKEN: 0.4508934020996094\n",
            "EPOCH: 31, CURRENT LOSS: -0.5039542317390442, TIME TAKEN: 0.45117807388305664\n",
            "EPOCH: 32, CURRENT LOSS: -0.5026226043701172, TIME TAKEN: 0.4485747814178467\n",
            "EPOCH: 33, CURRENT LOSS: -0.5020244121551514, TIME TAKEN: 0.4502720832824707\n",
            "EPOCH: 34, CURRENT LOSS: -0.5054582357406616, TIME TAKEN: 0.4481372833251953\n",
            "EPOCH: 35, CURRENT LOSS: -0.501063346862793, TIME TAKEN: 0.4498777389526367\n",
            "EPOCH: 36, CURRENT LOSS: -0.5038675665855408, TIME TAKEN: 0.45182037353515625\n",
            "EPOCH: 37, CURRENT LOSS: -0.5048236846923828, TIME TAKEN: 0.4497842788696289\n",
            "EPOCH: 38, CURRENT LOSS: -0.49988898634910583, TIME TAKEN: 0.4505472183227539\n",
            "EPOCH: 39, CURRENT LOSS: -0.5054515600204468, TIME TAKEN: 0.45276522636413574\n",
            "EPOCH: 40, CURRENT LOSS: -0.5032795071601868, TIME TAKEN: 0.44785237312316895\n",
            "EPOCH: 41, CURRENT LOSS: -0.5049964189529419, TIME TAKEN: 0.4488868713378906\n",
            "EPOCH: 42, CURRENT LOSS: -0.5089753866195679, TIME TAKEN: 0.45139598846435547\n",
            "EPOCH: 43, CURRENT LOSS: -0.5038384199142456, TIME TAKEN: 0.450606107711792\n",
            "EPOCH: 44, CURRENT LOSS: -0.5045546293258667, TIME TAKEN: 0.4518275260925293\n",
            "EPOCH: 45, CURRENT LOSS: -0.5071643590927124, TIME TAKEN: 0.45147252082824707\n",
            "EPOCH: 46, CURRENT LOSS: -0.5025727152824402, TIME TAKEN: 0.4515206813812256\n",
            "EPOCH: 47, CURRENT LOSS: -0.5012397170066833, TIME TAKEN: 0.4516422748565674\n",
            "EPOCH: 48, CURRENT LOSS: -0.503463864326477, TIME TAKEN: 0.4528920650482178\n",
            "EPOCH: 49, CURRENT LOSS: -0.506950855255127, TIME TAKEN: 0.45183515548706055\n",
            "EPOCH: 50, CURRENT LOSS: -0.49856895208358765, TIME TAKEN: 0.4514012336730957\n",
            "EPOCH: 51, CURRENT LOSS: -0.5032293796539307, TIME TAKEN: 0.45053791999816895\n",
            "EPOCH: 52, CURRENT LOSS: -0.5069357752799988, TIME TAKEN: 0.45121121406555176\n",
            "EPOCH: 53, CURRENT LOSS: -0.5045557022094727, TIME TAKEN: 0.45302295684814453\n",
            "EPOCH: 54, CURRENT LOSS: -0.503960907459259, TIME TAKEN: 0.4528312683105469\n",
            "EPOCH: 55, CURRENT LOSS: -0.5050489902496338, TIME TAKEN: 0.45236635208129883\n",
            "EPOCH: 56, CURRENT LOSS: -0.505954384803772, TIME TAKEN: 0.45384907722473145\n",
            "EPOCH: 57, CURRENT LOSS: -0.5045197606086731, TIME TAKEN: 0.45169639587402344\n",
            "EPOCH: 58, CURRENT LOSS: -0.5050182342529297, TIME TAKEN: 0.4517948627471924\n",
            "EPOCH: 59, CURRENT LOSS: -0.5051239132881165, TIME TAKEN: 0.44893670082092285\n",
            "EPOCH: 60, CURRENT LOSS: -0.5027788281440735, TIME TAKEN: 0.44997310638427734\n",
            "EPOCH: 61, CURRENT LOSS: -0.5038354992866516, TIME TAKEN: 0.4504365921020508\n",
            "EPOCH: 62, CURRENT LOSS: -0.5081614255905151, TIME TAKEN: 0.45092058181762695\n",
            "EPOCH: 63, CURRENT LOSS: -0.5006944537162781, TIME TAKEN: 0.44896841049194336\n",
            "EPOCH: 64, CURRENT LOSS: -0.5030531287193298, TIME TAKEN: 0.4492936134338379\n",
            "EPOCH: 65, CURRENT LOSS: -0.5015885829925537, TIME TAKEN: 0.4492964744567871\n",
            "EPOCH: 66, CURRENT LOSS: -0.5015873908996582, TIME TAKEN: 0.4502980709075928\n",
            "EPOCH: 67, CURRENT LOSS: -0.5051747560501099, TIME TAKEN: 0.45088982582092285\n",
            "EPOCH: 68, CURRENT LOSS: -0.5019673109054565, TIME TAKEN: 0.45100831985473633\n",
            "EPOCH: 69, CURRENT LOSS: -0.5112201571464539, TIME TAKEN: 0.45012950897216797\n",
            "EPOCH: 70, CURRENT LOSS: -0.5024071931838989, TIME TAKEN: 0.4510316848754883\n",
            "EPOCH: 71, CURRENT LOSS: -0.5035004615783691, TIME TAKEN: 0.4495277404785156\n",
            "EPOCH: 72, CURRENT LOSS: -0.5122259855270386, TIME TAKEN: 0.44980716705322266\n",
            "EPOCH: 73, CURRENT LOSS: -0.508781909942627, TIME TAKEN: 0.4504363536834717\n",
            "EPOCH: 74, CURRENT LOSS: -0.4983541667461395, TIME TAKEN: 0.44916725158691406\n",
            "EPOCH: 75, CURRENT LOSS: -0.5082685351371765, TIME TAKEN: 0.45269131660461426\n",
            "EPOCH: 76, CURRENT LOSS: -0.5025943517684937, TIME TAKEN: 0.45388054847717285\n",
            "EPOCH: 77, CURRENT LOSS: -0.504331648349762, TIME TAKEN: 0.4536561965942383\n",
            "EPOCH: 78, CURRENT LOSS: -0.505886435508728, TIME TAKEN: 0.45154595375061035\n",
            "EPOCH: 79, CURRENT LOSS: -0.5084790587425232, TIME TAKEN: 0.4549832344055176\n",
            "EPOCH: 80, CURRENT LOSS: -0.5064874887466431, TIME TAKEN: 0.4527161121368408\n",
            "EPOCH: 81, CURRENT LOSS: -0.5056970119476318, TIME TAKEN: 0.45482754707336426\n",
            "EPOCH: 82, CURRENT LOSS: -0.5066196322441101, TIME TAKEN: 0.4544491767883301\n",
            "EPOCH: 83, CURRENT LOSS: -0.5052781105041504, TIME TAKEN: 0.45304131507873535\n",
            "EPOCH: 84, CURRENT LOSS: -0.5056039094924927, TIME TAKEN: 0.45298075675964355\n",
            "EPOCH: 85, CURRENT LOSS: -0.5059389472007751, TIME TAKEN: 0.4532015323638916\n",
            "EPOCH: 86, CURRENT LOSS: -0.5080311894416809, TIME TAKEN: 0.45560383796691895\n",
            "EPOCH: 87, CURRENT LOSS: -0.5057777762413025, TIME TAKEN: 0.453707218170166\n",
            "EPOCH: 88, CURRENT LOSS: -0.5071638822555542, TIME TAKEN: 0.45517468452453613\n",
            "EPOCH: 89, CURRENT LOSS: -0.506800651550293, TIME TAKEN: 0.4526093006134033\n",
            "EPOCH: 90, CURRENT LOSS: -0.5054603219032288, TIME TAKEN: 0.452833890914917\n",
            "EPOCH: 91, CURRENT LOSS: -0.5111156105995178, TIME TAKEN: 0.45360279083251953\n",
            "EPOCH: 92, CURRENT LOSS: -0.5053916573524475, TIME TAKEN: 0.45566606521606445\n",
            "EPOCH: 93, CURRENT LOSS: -0.5086716413497925, TIME TAKEN: 0.45389509201049805\n",
            "EPOCH: 94, CURRENT LOSS: -0.5118634104728699, TIME TAKEN: 0.4554009437561035\n",
            "EPOCH: 95, CURRENT LOSS: -0.5093792080879211, TIME TAKEN: 0.45311760902404785\n",
            "EPOCH: 96, CURRENT LOSS: -0.5112221837043762, TIME TAKEN: 0.4561328887939453\n",
            "EPOCH: 97, CURRENT LOSS: -0.5042496919631958, TIME TAKEN: 0.4550795555114746\n",
            "EPOCH: 98, CURRENT LOSS: -0.5122799873352051, TIME TAKEN: 0.4535403251647949\n",
            "EPOCH: 99, CURRENT LOSS: -0.5088303089141846, TIME TAKEN: 0.45596885681152344\n",
            "EPOCH: 100, CURRENT LOSS: -0.5085503458976746, TIME TAKEN: 0.4541201591491699\n",
            "EPOCH: 101, CURRENT LOSS: -0.5066194534301758, TIME TAKEN: 0.45711708068847656\n",
            "EPOCH: 102, CURRENT LOSS: -0.5125677585601807, TIME TAKEN: 0.4560849666595459\n",
            "EPOCH: 103, CURRENT LOSS: -0.5138644576072693, TIME TAKEN: 0.4542999267578125\n",
            "EPOCH: 104, CURRENT LOSS: -0.5141671299934387, TIME TAKEN: 0.4528312683105469\n",
            "EPOCH: 105, CURRENT LOSS: -0.5151414275169373, TIME TAKEN: 0.45676636695861816\n",
            "EPOCH: 106, CURRENT LOSS: -0.5061237215995789, TIME TAKEN: 0.45435452461242676\n",
            "EPOCH: 107, CURRENT LOSS: -0.5061644911766052, TIME TAKEN: 0.4537036418914795\n",
            "EPOCH: 108, CURRENT LOSS: -0.5041345953941345, TIME TAKEN: 0.4549391269683838\n",
            "EPOCH: 109, CURRENT LOSS: -0.5042591691017151, TIME TAKEN: 0.4534611701965332\n",
            "EPOCH: 110, CURRENT LOSS: -0.5029013752937317, TIME TAKEN: 0.45308446884155273\n",
            "EPOCH: 111, CURRENT LOSS: -0.5052974820137024, TIME TAKEN: 0.4547872543334961\n",
            "EPOCH: 112, CURRENT LOSS: -0.5076823234558105, TIME TAKEN: 0.45427608489990234\n",
            "EPOCH: 113, CURRENT LOSS: -0.5104011297225952, TIME TAKEN: 0.45328569412231445\n",
            "EPOCH: 114, CURRENT LOSS: -0.5056902170181274, TIME TAKEN: 0.454331636428833\n",
            "EPOCH: 115, CURRENT LOSS: -0.5098483562469482, TIME TAKEN: 0.45325469970703125\n",
            "EPOCH: 116, CURRENT LOSS: -0.5065052509307861, TIME TAKEN: 0.4561309814453125\n",
            "EPOCH: 117, CURRENT LOSS: -0.5071320533752441, TIME TAKEN: 0.4574754238128662\n",
            "EPOCH: 118, CURRENT LOSS: -0.5064826011657715, TIME TAKEN: 0.453693151473999\n",
            "EPOCH: 119, CURRENT LOSS: -0.5110272765159607, TIME TAKEN: 0.4544694423675537\n",
            "EPOCH: 120, CURRENT LOSS: -0.5088841319084167, TIME TAKEN: 0.4546363353729248\n",
            "EPOCH: 121, CURRENT LOSS: -0.5058833956718445, TIME TAKEN: 0.45462656021118164\n",
            "EPOCH: 122, CURRENT LOSS: -0.5108821392059326, TIME TAKEN: 0.45714879035949707\n",
            "EPOCH: 123, CURRENT LOSS: -0.5007511377334595, TIME TAKEN: 0.45360779762268066\n",
            "EPOCH: 124, CURRENT LOSS: -0.5047996044158936, TIME TAKEN: 0.4537546634674072\n",
            "EPOCH: 125, CURRENT LOSS: -0.502504289150238, TIME TAKEN: 0.4550151824951172\n",
            "EPOCH: 126, CURRENT LOSS: -0.5059800744056702, TIME TAKEN: 0.4547874927520752\n",
            "EPOCH: 127, CURRENT LOSS: -0.5119501352310181, TIME TAKEN: 0.45531678199768066\n",
            "EPOCH: 128, CURRENT LOSS: -0.5092626214027405, TIME TAKEN: 0.4555633068084717\n",
            "EPOCH: 129, CURRENT LOSS: -0.5035448670387268, TIME TAKEN: 0.4525625705718994\n",
            "EPOCH: 130, CURRENT LOSS: -0.5025718212127686, TIME TAKEN: 0.45430684089660645\n",
            "EPOCH: 131, CURRENT LOSS: -0.5115877389907837, TIME TAKEN: 0.45658278465270996\n",
            "EPOCH: 132, CURRENT LOSS: -0.5166482329368591, TIME TAKEN: 0.4558451175689697\n",
            "EPOCH: 133, CURRENT LOSS: -0.513032078742981, TIME TAKEN: 0.4555375576019287\n",
            "EPOCH: 134, CURRENT LOSS: -0.5010201930999756, TIME TAKEN: 0.45609211921691895\n",
            "EPOCH: 135, CURRENT LOSS: -0.50211101770401, TIME TAKEN: 0.45062971115112305\n",
            "EPOCH: 136, CURRENT LOSS: -0.5112337470054626, TIME TAKEN: 0.45429062843322754\n",
            "EPOCH: 137, CURRENT LOSS: -0.5148807168006897, TIME TAKEN: 0.457599401473999\n",
            "EPOCH: 138, CURRENT LOSS: -0.5107344388961792, TIME TAKEN: 0.4551665782928467\n",
            "EPOCH: 139, CURRENT LOSS: -0.5037214756011963, TIME TAKEN: 0.4558725357055664\n",
            "EPOCH: 140, CURRENT LOSS: -0.5029904842376709, TIME TAKEN: 0.4562561511993408\n",
            "EPOCH: 141, CURRENT LOSS: -0.5092392563819885, TIME TAKEN: 0.45491909980773926\n",
            "EPOCH: 142, CURRENT LOSS: -0.5081460475921631, TIME TAKEN: 0.4553515911102295\n",
            "EPOCH: 143, CURRENT LOSS: -0.5088632106781006, TIME TAKEN: 0.4568333625793457\n",
            "EPOCH: 144, CURRENT LOSS: -0.5102893710136414, TIME TAKEN: 0.4553408622741699\n",
            "EPOCH: 145, CURRENT LOSS: -0.5112911462783813, TIME TAKEN: 0.4557654857635498\n",
            "EPOCH: 146, CURRENT LOSS: -0.5086959600448608, TIME TAKEN: 0.4552760124206543\n",
            "EPOCH: 147, CURRENT LOSS: -0.49807560443878174, TIME TAKEN: 0.45302271842956543\n",
            "EPOCH: 148, CURRENT LOSS: -0.5109853148460388, TIME TAKEN: 0.45703959465026855\n",
            "EPOCH: 149, CURRENT LOSS: -0.5072780847549438, TIME TAKEN: 0.45581626892089844\n",
            "EPOCH: 150, CURRENT LOSS: -0.5017385482788086, TIME TAKEN: 0.4545276165008545\n",
            "EPOCH: 151, CURRENT LOSS: -0.5123016834259033, TIME TAKEN: 0.45648622512817383\n",
            "EPOCH: 152, CURRENT LOSS: -0.5134525299072266, TIME TAKEN: 0.4545707702636719\n",
            "EPOCH: 153, CURRENT LOSS: -0.5156663656234741, TIME TAKEN: 0.4552135467529297\n",
            "EPOCH: 154, CURRENT LOSS: -0.5122217535972595, TIME TAKEN: 0.45546388626098633\n",
            "EPOCH: 155, CURRENT LOSS: -0.5102892518043518, TIME TAKEN: 0.4539973735809326\n",
            "EPOCH: 156, CURRENT LOSS: -0.5094024538993835, TIME TAKEN: 0.455507755279541\n",
            "EPOCH: 157, CURRENT LOSS: -0.5101613402366638, TIME TAKEN: 0.4553947448730469\n",
            "EPOCH: 158, CURRENT LOSS: -0.5176339745521545, TIME TAKEN: 0.4537780284881592\n",
            "EPOCH: 159, CURRENT LOSS: -0.5043184757232666, TIME TAKEN: 0.4552779197692871\n",
            "EPOCH: 160, CURRENT LOSS: -0.5039573907852173, TIME TAKEN: 0.45418763160705566\n",
            "EPOCH: 161, CURRENT LOSS: -0.4931526184082031, TIME TAKEN: 0.4548337459564209\n",
            "EPOCH: 162, CURRENT LOSS: -0.5153757929801941, TIME TAKEN: 0.4556128978729248\n",
            "EPOCH: 163, CURRENT LOSS: -0.5109735131263733, TIME TAKEN: 0.4548921585083008\n",
            "EPOCH: 164, CURRENT LOSS: -0.5065867900848389, TIME TAKEN: 0.4535255432128906\n",
            "EPOCH: 165, CURRENT LOSS: -0.5050250887870789, TIME TAKEN: 0.4555063247680664\n",
            "EPOCH: 166, CURRENT LOSS: -0.507297158241272, TIME TAKEN: 0.45641136169433594\n",
            "EPOCH: 167, CURRENT LOSS: -0.5144184231758118, TIME TAKEN: 0.4549386501312256\n",
            "EPOCH: 168, CURRENT LOSS: -0.5210787057876587, TIME TAKEN: 0.4568183422088623\n",
            "EPOCH: 169, CURRENT LOSS: -0.5049465298652649, TIME TAKEN: 0.45553159713745117\n",
            "EPOCH: 170, CURRENT LOSS: -0.510535717010498, TIME TAKEN: 0.4555370807647705\n",
            "EPOCH: 171, CURRENT LOSS: -0.5062096118927002, TIME TAKEN: 0.4572329521179199\n",
            "EPOCH: 172, CURRENT LOSS: -0.5088484287261963, TIME TAKEN: 0.45421338081359863\n",
            "EPOCH: 173, CURRENT LOSS: -0.5193082690238953, TIME TAKEN: 0.455061674118042\n",
            "EPOCH: 174, CURRENT LOSS: -0.5136829018592834, TIME TAKEN: 0.457810640335083\n",
            "EPOCH: 175, CURRENT LOSS: -0.5159615874290466, TIME TAKEN: 0.45452117919921875\n",
            "EPOCH: 176, CURRENT LOSS: -0.5086873173713684, TIME TAKEN: 0.45331454277038574\n",
            "EPOCH: 177, CURRENT LOSS: -0.5022850036621094, TIME TAKEN: 0.4548952579498291\n",
            "EPOCH: 178, CURRENT LOSS: -0.5073093175888062, TIME TAKEN: 0.45351386070251465\n",
            "EPOCH: 179, CURRENT LOSS: -0.5116735100746155, TIME TAKEN: 0.45467257499694824\n",
            "EPOCH: 180, CURRENT LOSS: -0.5102822780609131, TIME TAKEN: 0.4569694995880127\n",
            "EPOCH: 181, CURRENT LOSS: -0.5037063956260681, TIME TAKEN: 0.45401477813720703\n",
            "EPOCH: 182, CURRENT LOSS: -0.5106503963470459, TIME TAKEN: 0.45720767974853516\n",
            "EPOCH: 183, CURRENT LOSS: -0.5070207118988037, TIME TAKEN: 0.4559030532836914\n",
            "EPOCH: 184, CURRENT LOSS: -0.505290687084198, TIME TAKEN: 0.4552640914916992\n",
            "EPOCH: 185, CURRENT LOSS: -0.511191725730896, TIME TAKEN: 0.4574241638183594\n",
            "EPOCH: 186, CURRENT LOSS: -0.5114656090736389, TIME TAKEN: 0.4547419548034668\n",
            "EPOCH: 187, CURRENT LOSS: -0.512237012386322, TIME TAKEN: 0.4538075923919678\n",
            "EPOCH: 188, CURRENT LOSS: -0.5196991562843323, TIME TAKEN: 0.45532965660095215\n",
            "EPOCH: 189, CURRENT LOSS: -0.5260647535324097, TIME TAKEN: 0.4568307399749756\n",
            "EPOCH: 190, CURRENT LOSS: -0.5177236199378967, TIME TAKEN: 0.45474743843078613\n",
            "EPOCH: 191, CURRENT LOSS: -0.514896035194397, TIME TAKEN: 0.4543790817260742\n",
            "EPOCH: 192, CURRENT LOSS: -0.5251747965812683, TIME TAKEN: 0.4527561664581299\n",
            "EPOCH: 193, CURRENT LOSS: -0.5138541460037231, TIME TAKEN: 0.4555516242980957\n",
            "EPOCH: 194, CURRENT LOSS: -0.5182880163192749, TIME TAKEN: 0.4578115940093994\n",
            "EPOCH: 195, CURRENT LOSS: -0.5119988918304443, TIME TAKEN: 0.45505475997924805\n",
            "EPOCH: 196, CURRENT LOSS: -0.5020979046821594, TIME TAKEN: 0.4534950256347656\n",
            "EPOCH: 197, CURRENT LOSS: -0.516501247882843, TIME TAKEN: 0.4596285820007324\n",
            "EPOCH: 198, CURRENT LOSS: -0.5025798678398132, TIME TAKEN: 0.4543159008026123\n",
            "EPOCH: 199, CURRENT LOSS: -0.5224784016609192, TIME TAKEN: 0.45757222175598145\n",
            "EPOCH: 200, CURRENT LOSS: -0.5089046955108643, TIME TAKEN: 0.45740532875061035\n",
            "EPOCH: 201, CURRENT LOSS: -0.518875002861023, TIME TAKEN: 0.454801082611084\n",
            "EPOCH: 202, CURRENT LOSS: -0.5205228924751282, TIME TAKEN: 0.45506763458251953\n",
            "EPOCH: 203, CURRENT LOSS: -0.5132904648780823, TIME TAKEN: 0.4560391902923584\n",
            "EPOCH: 204, CURRENT LOSS: -0.5116462707519531, TIME TAKEN: 0.45522522926330566\n",
            "EPOCH: 205, CURRENT LOSS: -0.5174936652183533, TIME TAKEN: 0.45752501487731934\n",
            "EPOCH: 206, CURRENT LOSS: -0.5164926648139954, TIME TAKEN: 0.45645928382873535\n",
            "EPOCH: 207, CURRENT LOSS: -0.5101927518844604, TIME TAKEN: 0.4540128707885742\n",
            "EPOCH: 208, CURRENT LOSS: -0.5150901079177856, TIME TAKEN: 0.45374298095703125\n",
            "EPOCH: 209, CURRENT LOSS: -0.510297417640686, TIME TAKEN: 0.4562530517578125\n",
            "EPOCH: 210, CURRENT LOSS: -0.5226165056228638, TIME TAKEN: 0.4557790756225586\n",
            "EPOCH: 211, CURRENT LOSS: -0.5074166655540466, TIME TAKEN: 0.4558393955230713\n",
            "EPOCH: 212, CURRENT LOSS: -0.5153404474258423, TIME TAKEN: 0.4547719955444336\n",
            "EPOCH: 213, CURRENT LOSS: -0.521499752998352, TIME TAKEN: 0.4535791873931885\n",
            "EPOCH: 214, CURRENT LOSS: -0.49864089488983154, TIME TAKEN: 0.4563627243041992\n",
            "EPOCH: 215, CURRENT LOSS: -0.520776629447937, TIME TAKEN: 0.45647740364074707\n",
            "EPOCH: 216, CURRENT LOSS: -0.5118923187255859, TIME TAKEN: 0.4547288417816162\n",
            "EPOCH: 217, CURRENT LOSS: -0.5099396109580994, TIME TAKEN: 0.4550807476043701\n",
            "EPOCH: 218, CURRENT LOSS: -0.5011618733406067, TIME TAKEN: 0.45714759826660156\n",
            "EPOCH: 219, CURRENT LOSS: -0.5122211575508118, TIME TAKEN: 0.45499253273010254\n",
            "EPOCH: 220, CURRENT LOSS: -0.5106019377708435, TIME TAKEN: 0.4567074775695801\n",
            "EPOCH: 221, CURRENT LOSS: -0.506676971912384, TIME TAKEN: 0.45465826988220215\n",
            "EPOCH: 222, CURRENT LOSS: -0.5295168161392212, TIME TAKEN: 0.45386314392089844\n",
            "EPOCH: 223, CURRENT LOSS: -0.5021834373474121, TIME TAKEN: 0.4564070701599121\n",
            "EPOCH: 224, CURRENT LOSS: -0.5259978175163269, TIME TAKEN: 0.45481133460998535\n",
            "EPOCH: 225, CURRENT LOSS: -0.5290178060531616, TIME TAKEN: 0.4543631076812744\n",
            "EPOCH: 226, CURRENT LOSS: -0.5056479573249817, TIME TAKEN: 0.45600414276123047\n",
            "EPOCH: 227, CURRENT LOSS: -0.5141651034355164, TIME TAKEN: 0.454639196395874\n",
            "EPOCH: 228, CURRENT LOSS: -0.5231056809425354, TIME TAKEN: 0.4561030864715576\n",
            "EPOCH: 229, CURRENT LOSS: -0.5092115998268127, TIME TAKEN: 0.45470213890075684\n",
            "EPOCH: 230, CURRENT LOSS: -0.5121859312057495, TIME TAKEN: 0.4552741050720215\n",
            "EPOCH: 231, CURRENT LOSS: -0.5032329559326172, TIME TAKEN: 0.4561929702758789\n",
            "EPOCH: 232, CURRENT LOSS: -0.5146663784980774, TIME TAKEN: 0.455157995223999\n",
            "EPOCH: 233, CURRENT LOSS: -0.5124595761299133, TIME TAKEN: 0.4542660713195801\n",
            "EPOCH: 234, CURRENT LOSS: -0.5252310633659363, TIME TAKEN: 0.4538750648498535\n",
            "EPOCH: 235, CURRENT LOSS: -0.5081177353858948, TIME TAKEN: 0.4542562961578369\n",
            "EPOCH: 236, CURRENT LOSS: -0.5033445358276367, TIME TAKEN: 0.45747995376586914\n",
            "EPOCH: 237, CURRENT LOSS: -0.5050114393234253, TIME TAKEN: 0.45493626594543457\n",
            "EPOCH: 238, CURRENT LOSS: -0.5164088010787964, TIME TAKEN: 0.4540383815765381\n",
            "EPOCH: 239, CURRENT LOSS: -0.5191875696182251, TIME TAKEN: 0.45424556732177734\n",
            "EPOCH: 240, CURRENT LOSS: -0.5150308012962341, TIME TAKEN: 0.4572620391845703\n",
            "EPOCH: 241, CURRENT LOSS: -0.5296629667282104, TIME TAKEN: 0.4551198482513428\n",
            "EPOCH: 242, CURRENT LOSS: -0.5094020366668701, TIME TAKEN: 0.4553961753845215\n",
            "EPOCH: 243, CURRENT LOSS: -0.5071532130241394, TIME TAKEN: 0.45544886589050293\n",
            "EPOCH: 244, CURRENT LOSS: -0.5069721341133118, TIME TAKEN: 0.45388317108154297\n",
            "EPOCH: 245, CURRENT LOSS: -0.5168951153755188, TIME TAKEN: 0.4556589126586914\n",
            "EPOCH: 246, CURRENT LOSS: -0.5138522386550903, TIME TAKEN: 0.45609283447265625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDP0j9klAf9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mdl_1a = torch.load('model1A.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A0ZU0noHFDSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validate(mdl_1a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}