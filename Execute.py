# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Sirius-Aabhas/CS69002_9A_18CS60R55/blob/master/Execute.ipynb
"""

import torch
import pandas as pd
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import string
from torch.autograd import Variable
import sys
import pickle 


def make_bow_vector(sentence, word_to_ix):
    # create a vector of zeros of vocab size = len(word_to_idx)
    vec = torch.zeros(len(word_to_ix))
    for word in sentence.split():
        if word not in word_to_ix:
            #raise ValueError('Word',word,' not present in the dictionary. Sorry!')
            vec[word_to_ix['<UNKNOWN>']]+=1
        else:
            vec[word_to_ix[word]]+=1
    return vec.view(1, -1)

"""##Task1"""

class Task1A(nn.Module):
    def __init__(self, vocab_size):
        super(Task1A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 50)
        self.lin2 = nn.Linear(50, 2)

    def forward(self, x):
        x = self.lin1(x)
        return F.softmax(self.lin2(x))
    
class Task1B(nn.Module):
    def __init__(self, vocab_size):
        super(Task1B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 100)
        self.lin2 = nn.Linear(100, 2)

    def forward(self, x):
        x = self.lin1(x)
        return F.softmax(self.lin2(x))
    
class Task1C(nn.Module):
    def __init__(self, vocab_size):
        super(Task1C, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 200)
        self.lin2 = nn.Linear(200, 2)

    def forward(self, x):
        x = self.lin1(x)
        return F.softmax(self.lin2(x))
    
class Task1D(nn.Module):
    def __init__(self, vocab_size):
        super(Task1D, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 500)
        self.lin2 = nn.Linear(500, 2)

    def forward(self, x):
        x = self.lin1(x)
        return F.softmax(self.lin2(x))

"""##Task2"""

class Task2A(nn.Module):
    def __init__(self, vocab_size):
        super(Task2A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 10)
        self.lin2 = nn.Linear(10, 10)
        self.lin3 = nn.Linear(10, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))
    
class Task2B(nn.Module):
    def __init__(self, vocab_size):
        super(Task2B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 20)
        self.lin2 = nn.Linear(20, 10)
        self.lin3 = nn.Linear(10, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))
    
class Task2C(nn.Module):
    def __init__(self, vocab_size):
        super(Task2C, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 30)
        self.lin3 = nn.Linear(30, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))
    
class Task2D(nn.Module):
    def __init__(self, vocab_size):
        super(Task2D, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 50)
        self.lin2 = nn.Linear(50, 50)
        self.lin3 = nn.Linear(50, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))
    
class Task2E(nn.Module):
    def __init__(self, vocab_size):
        super(Task2E, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 100)
        self.lin2 = nn.Linear(100, 50)
        self.lin3 = nn.Linear(50, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))

"""##Task3"""

class Task3A(nn.Module):
    def __init__(self, vocab_size):
        super(Task3A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 100)
        self.lin2 = nn.Linear(100, 50)
        self.lin3 = nn.Linear(50, 10)
        self.lin4 = nn.Linear(10, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        x = self.lin3(x)
        return F.softmax(self.lin4(x))
    
class Task3B(nn.Module):
    def __init__(self, vocab_size):
        super(Task3B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 200)
        self.lin2 = nn.Linear(200, 100)
        self.lin3 = nn.Linear(100, 10)
        self.lin4 = nn.Linear(10, 2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        x = self.lin3(x)
        return F.softmax(self.lin4(x))

"""##Task4"""

class Task4A(nn.Module):
    def __init__(self, vocab_size):
        super(Task4A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.lin2(x)
        x = self.drop_layer(x)
        x = self.lin3(x)
        return F.softmax(self.lin4(x))
    
class Task4B(nn.Module):
    def __init__(self, vocab_size):
        super(Task4B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 100)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin2 = nn.Linear(100, 100)
        self.lin3 = nn.Linear(100,2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.drop_layer(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))

class Task4C(nn.Module):
    def __init__(self, vocab_size):
        super(Task4C, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 100)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin2 = nn.Linear(100, 10)
        self.lin3 = nn.Linear(10,2)

    def forward(self, x): 
        x = self.lin1(x)
        x = self.drop_layer(x)
        x = self.lin2(x)
        return F.softmax(self.lin3(x))

"""##Task5"""

class Task5A(nn.Module):
    def __init__(self, vocab_size):
        super(Task5A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = self.drop_layer(x)
        x = F.relu(self.lin3(x))
        return F.softmax(self.lin4(x))
    
class Task5B(nn.Module):
    def __init__(self, vocab_size):
        super(Task5B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.tanh(self.lin1(x))
        x = F.tanh(self.lin2(x))
        x = self.drop_layer(x)
        x = F.tanh(self.lin3(x))
        return F.softmax(self.lin4(x))
    
class Task5C(nn.Module):
    def __init__(self, vocab_size):
        super(Task5C, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.sigmoid(self.lin1(x))
        x = F.sigmoid(self.lin2(x))
        x = self.drop_layer(x)
        x = F.sigmoid(self.lin3(x))
        return F.softmax(self.lin4(x))

"""##Task6"""

class Task6(nn.Module):
    def __init__(self, vocab_size):
        super(Task6, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.drop_layer = nn.Dropout(p=0.4)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x):
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = self.drop_layer(x)
        x = F.relu(self.lin3(x))
        return F.softmax(self.lin4(x))
		
"""##Task7"""

class Task7A(nn.Module):
    def __init__(self, vocab_size):
        super(Task7A, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = F.relu(self.lin3(x))
        return F.softmax(self.lin4(x))
    
class Task7B(nn.Module):
    def __init__(self, vocab_size):
        super(Task7B, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = F.relu(self.lin3(x))
        return F.softmax(self.lin4(x))

class Task7C(nn.Module):
    def __init__(self, vocab_size):
        super(Task7C, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 30)
        self.lin2 = nn.Linear(30, 20)
        self.lin3 = nn.Linear(20, 10)
        self.lin4 = nn.Linear(10,2)

    def forward(self, x): 
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = F.relu(self.lin3(x))
        return F.softmax(self.lin4(x))

"""##Main code"""

def predict_and_save(model, file, data_test):
    model = model.cpu()
    preds = []
    for instance in data_test:
        bow_vec = Variable(make_bow_vector(instance, word_to_ix))
        logprobs = model(bow_vec)
        #print(logprobs)
        pred = np.argmax(logprobs.data.cpu().numpy())
        preds.append(pred)    
    
    with open(file, 'w') as f:
        for pred in list(preds):
            f.write(str(pred) + '\n')


if __name__ == '__main__':
	if len(sys.argv) != 3:
		print("Please give the cmd line arguments mentioned in the assignment!!")
		sys.exit()
		
	model = sys.argv[1]
	test_file = sys.argv[2]
	
	if 'Task6' in model:
		with open('bigram_dictionary.pkl', 'rb') as f:
			word_to_ix = pickle.load(f)
	else:
		with open('word_dictionary.pkl', 'rb') as f:
			word_to_ix = pickle.load(f)
	
	
	with open(test_file) as f:
		text_reviews_test = f.readlines()

	text_reviews_test = [x.lower() for x in text_reviews_test]

	mdl = torch.load(model)
	out_file = mdl._get_name() + '_pred.csv'
	predict_and_save(mdl, out_file, text_reviews_test)
